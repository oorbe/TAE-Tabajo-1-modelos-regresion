---
title: "modelos"
author: "Oliver Rodriguez"
date: "31/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(fastDummies)
```

```{r}
datos <- read.csv('datos.csv')
caracters <- c('vive_con_conyuge','nvl_educacion_papa','nvl_educacion_mama','rasgos','campesino','A_EPS','SEG_SOCIAL_SALUD','vivienda_es','REGION','TIPO_VIVIENDA')
datos[,caracters] <- apply(datos[,caracters], 2, as.character)
datos <- datos[,-1]

caracteristicas <- dummy_cols(datos, select_columns = c("vive_con_conyuge",
                                                                    "nvl_educacion_papa",
                                                                    "nvl_educacion_mama",
                                                                    "rasgos",
                                                                    "campesino",
                                                                      'actividad_semana_pasada',
                                                        'A_EPS','SEG_SOCIAL_SALUD','vivienda_es','REGION',
                                                        'TIPO_VIVIENDA' 
                                                        ), 
                                remove_selected_columns = T)
```


```{r}
datos2 <- scale(datos, center = TRUE, scale = TRUE)
centro<-attr(datos2,"scaled:center")#apply(datos,2,mean)
escala<-attr(datos2,"scaled:scale")#apply(datos,2,sd)
```

```{r}
#pairs(datos[,1:8],)
```


modelo de regresion lineal:
```{r}
library(leaps)
```


Utilizamos regsubset para la selección de las mejore varibles:
```{r}
reg_full_fit <- regsubsets(n~.,data = datos, nvmax=34)
sumary <- summary(reg_full_fit)
sumary
```

Viendo el R^2 con una sola variable será de 5.96% pero usandolas todas maximo optiene un 18.8% 
```{r}
sumary$rsq
```

Plotting RSS, adjusted R2, Cp, and BIC
```{r}
par(mfrow=c(2,2))
plot(sumary$rsq ,xlab=" Number of Variables ",ylab=" R^2", type="o")
plot(sumary$adjr2 ,xlab=" Number of Variables ",ylab=" R^2 adjus", type="o")
plot(sumary$rss ,xlab=" Number of Variables ",ylab=" RSS", type="o")
plot(sumary$bic ,xlab=" Number of Variables ",ylab="BIC", type="o")

```

```{r}

```

Al parecer se en general se suficiente utilizar 10 variables para generalizar un buen modelo:


Forward and Backward y Selection:
Se puede observar que se obtienen las mismas varaibles para un tamaño adecuado de 10 en todos los metodos de selección:
```{r}
reg_forw_fit <- regsubsets(n~.,data = datos, nvmax=34, method = 'f')
reg_back_fit <- regsubsets(n~.,data = datos, nvmax=34, method = 'b')

coef(reg_full_fit, 10)
coef(reg_forw_fit, 10)
coef(reg_back_fit, 10)
```



Ahora vamos a realiza la seleccion de variables utilizando rich and lasso regretion:
```{r}
#install.packages('glmnet')
library(glmnet)
# matriz de diseño
x <- model.matrix(n~.,datos)[,-1]
y <- datos$n
```

# primero ridge
```{r}
grid <- 10^seq(10,-1,length=100)
ridge_mod <- glmnet(x,y,alpha = 0, lambda=grid)
plot(ridge_mod)
```

Al parece algunos coeficientes  tienden a cero a medida que se mueven los valores del lambda:
realizamos particionamiento de los datos en entrenamiento y prueba para realiza validacion cruzada y encontrar el mejor lambda:
```{r}
set.seed (1)
train=sample (1: nrow(x), nrow(x)/2)
test=(-train )
y.test=y[test]
```


Planteamos el modelo:
```{r}
ridge.mod =glmnet (x[train,],y[train],alpha =0, lambda =grid ,
thresh =1e-12)
ridge.pred=predict(ridge.mod ,s=4, newx=x[test ,])
mean(( ridge.pred -y.test)^2)
```

Se obtiene el mejor lambda y luego se calcula la predicción con el
```{r}
# ahora voy a hallar el mejor lambda
set.seed (1)
cv.out =cv.glmnet (x[train ,],y[train],alpha =0)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
ridge.pred=predict (ridge.mod ,s=bestlam ,newx=x[test ,])
sqrt(mean(( ridge.pred -y.test)^2))
```

```{r}
out=glmnet (x,y,alpha =0)
rigge.coef <- predict (out ,type="coefficients",s=bestlam )[1:19,]
round(rigge.coef[rigge.coef>0.0001 | rigge.coef<0 ],6)
```

# AHORA REALIZAMOS LASSO:
```{r}
lasso.mod =glmnet (x[train ,],y[train],alpha =1, lambda =grid)
plot(lasso.mod)
```
Aqui se observa que al aumentar los valores de lambda muchos de los parametros tieneden a cero.

```{r}
set.seed (1)
cv.out =cv.glmnet (x[train ,],y[train],alpha =1)
plot(cv.out)
bestlam =cv.out$lambda.min
lasso.pred=predict (lasso.mod ,s=bestlam ,newx=x[test ,])
sqrt(mean(( lasso.pred -y.test)^2))
```

```{r}
out=glmnet (x,y,alpha =1, lambda =grid)
lasso.coef=predict(out, type ="coefficients",s=bestlam )[1:19,]
lasso.coef[lasso.coef!=0]
```

# los resultado obenido por los diferentes metodos para la selección de variables son :
```{r}
# forward backward y stepwice, sus resultados me dan iguales:
coef(reg_full_fit, 10)
coef(reg_forw_fit, 10)
coef(reg_back_fit, 10)
```


```{r}
# ride: me deja muchas variables
round(rigge.coef[rigge.coef>0.005 | rigge.coef < 0],4)
print('\n')
# lasso
lasso.coef[lasso.coef!=0]
```

# division en entrenamiento y prueba:
```{r}
library(caret)
set.seed(1037637276)
intrain <- createDataPartition(y = datos$n, p= 0.85, list = FALSE)
training <- datos[intrain,]
testing <- datos[-intrain,]
```

modelo lienal con las variables previamente halladas por leaps:
```{r}
mod_leaps <- lm(n~., data=training[,c('n','edad','X1ER_HIJO_EDAD','TOT_DISPOSITIVOS','I_HOGAR',
                                     'gas_consumo','vive_con_conyuge','rasgos','actividad_semana_pasada','SEG_SOCIAL_SALUD')])
summary(mod_leaps)
pred_leaps <- predict(mod_leaps, newdata = testing)
rmse_leaps <- sqrt(mean((pred_leaps-testing$n)^2)); rmse_leaps
```

```{r}
mod_ridge <- lm(n~.,data = training[,c('n','edad','X1ER_HIJO_EDAD','TOT_DISPOSITIVOS','vive_con_conyuge','nvl_educacion_papa','nvl_educacion_mama','rasgos','campesino','actividad_semana_pasada')])
summary(mod_ridge)
pred_ridge <- predict(mod_ridge, newdata = testing)
rmse_ridge <- sqrt(mean((pred_ridge-testing$n)^2)); rmse_lasso
```

modelo hallado con lasso:
```{r}
mod_lasso <- lm(n~.,data = training[,c('n','edad','X1ER_HIJO_EDAD','vive_con_conyuge','rasgos')])
summary(mod_lasso)
pred_lasso <- predict(mod_lasso, newdata = testing)
rmse_lasso <- sqrt(mean((pred_lasso-testing$n)^2)); rmse_lasso
```

Podemos cocluir que el mejor modelo segun el RMSE de prueba es el conrtuido con los metodos de forward, backward y stepwyse, pero su diferencia es por poco.

Ahora será necesaria la validación de supuestos:

```{r}
par(mfrow=c(2,2))
plot(mod_leaps)
```

```{r}
par(mfrow=c(2,2))
plot(mod_lasso)
```

```{r}
par(mfrow=c(2,2))
plot(mod_ridge)
```

Concluimos que los modelo tienen serios problemas con el cumplimiento de los supuesto de homocedasticidad y de normalidad de los errores. Se intentaron plantear algunas transformaciones, pero no se  obseva un patro claro  en los gráficos de los errores para aplicar determinada transformación como una forma cuadrática o exponencial.

Hallamos box cox con el proposito de lograr cumplir el supuesto de normalidad pero, no puede ser hallada pues los valores de la variable respuesta deben ser positivos y hay muchos ceros.

```{r}
library(MASS)
#find optimal lambda for Box-Cox transformation 
bc <- boxcox(n~., data= datos[,c('n','edad','X1ER_HIJO_EDAD','vive_con_conyuge','rasgos')])
(lambda <- bc$x[which.max(bc$y)])

#fit new linear regression model using the Box-Cox transformation
new_model <- lm(((y^lambda-1)/lambda) ~ x)
```

# tree regression:

Ahora se realizará el ajuste del modelo con un valor cp(parametro de complejidad) pequeño, que nos ayudará a encontrar una profundidad adecuada para obtener el mejor arbol, es decir que tenga menor error de entrenamiento. Una cualidad de los arboles de regresión es que no hay que realizar una revia selección de varaibles, él tomara en su raíz las que considera más importantes, y la siguiente salidad de R nos mostrará los resultados de las varaibles seleccionadas:
```{r}
library(caret)
set.seed(1037637276)
intrain <- createDataPartition(y = datos$n, p= 0.85, list = FALSE)
training <- datos[intrain,]
testing <- datos[-intrain,]
    
library(rpart)
mod_tree <- rpart(n~.,  method = "anova", data=training, control = rpart.control(cp=.0001))
printcp(mod_tree)
```

A continuación, podaremos el árbol de regresión para encontrar el valor óptimo que se usará para cp (el parámetro de complejidad) que conduce al error de prueba más bajo.
```{r}
#identify best cp value to use
best <- mod_tree$cptable[which.min(mod_tree$cptable[,"xerror"]),"CP"]

#produce a pruned tree based on the best cp value
pruned_tree <- prune(mod_tree, cp=best)
```


para graficar... aunque no se alcanza a ver nada:
```{r}
library(rpart.plot)
#plot the pruned tree
prp(pruned_tree,
    faclen=0, #use full names for factor labels
    extra=1, #display number of obs. for each terminal node
    roundint=F, #don't round to integers in output
    digits=5) #display 5 decimal places in output
```


predicciones:
```{r}
p <- predict(mod_tree, testing, method = "anova")
# este es el error de prueba:
rmse_tree <- sqrt(mean((p-testing$n)^2));rmse_tree
predict(mod_tree, testing[1:4,], method = "anova")
```

finalmente comparamos los RMSE de los modelos:
```{r}
rmse_lasso
rmse_leaps
rmse_ridge
rmse_tree
```
numericamente los valores son muy similares, pero podriamos quedarnos con el modelo de arboles de desición pues no debe cumplir con supuestos, a diferencia de los demás que tienen serios problemas con sus supuestos.

